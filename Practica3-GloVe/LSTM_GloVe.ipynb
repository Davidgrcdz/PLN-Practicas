{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioeoy9Xc2bmQ"
      },
      "source": [
        "<p>\n",
        "<img src=\"../imgs/EII-ULPGC-logo.jpeg\" width=\"430px\" align=\"right\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psDJy37r2bmS"
      },
      "source": [
        "# **NOTEBOOK 13**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihZ5XDmw2bmT"
      },
      "source": [
        "# **Modelos del lenguaje basados en redes neuronales artificiales**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aucQR72r2bmT"
      },
      "source": [
        "## **Redes neuronales recurrentes (RNN)**\n",
        "\n",
        "### **Ejemplo de aplicación de un modelo LSTM a la clasificación de textos**\n",
        "\n",
        "#### **Dataset**\n",
        "\n",
        "El conjunto de datos \"AG_NEWS\" es un conjunto de datos de clasificación de texto ampliamente utilizado en el campo del procesamiento de lenguaje natural (NLP). Contiene noticias de diferentes categorías y se utiliza comúnmente para tareas de clasificación de texto. El conjunto de datos AG_NEWS consta de noticias de cuatro categorías principales, que son:\n",
        "\n",
        "1. **World**: Noticias sobre eventos y acontecimientos globales, como política internacional, relaciones internacionales y noticias mundiales en general.\n",
        "\n",
        "2. **Sports**: Noticias relacionadas con eventos deportivos, resultados de partidos, eventos deportivos nacionales e internacionales, etc.\n",
        "\n",
        "3. **Business**: Noticias relacionadas con el mundo de los negocios, finanzas, economía, empresas, informes de ganancias y otros temas económicos.\n",
        "\n",
        "4. **Sci/Tech**: Noticias relacionadas con ciencia y tecnología, incluyendo avances científicos, novedades tecnológicas, gadgets, investigaciones científicas y más.\n",
        "\n",
        "Cada instancia del conjunto de datos AG_NEWS generalmente consiste en un título y un cuerpo de una noticia, junto con una etiqueta que indica la categoría a la que pertenece."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XrB4iRR4Aqk",
        "outputId": "304b2059-9110-4de5-b91b-5126f9737bbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.5.1+cu121\n",
            "Uninstalling torch-2.5.1+cu121:\n",
            "  Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.0.1\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp310-cp310-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m453.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.15.2\n",
            "  Downloading https://download.pytorch.org/whl/torchtext-0.15.2%2Bcpu-cp310-cp310-linux_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1) (3.1.4)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Collecting torchdata==0.6.1 (from torchtext==0.15.2)\n",
            "  Downloading https://download.pytorch.org/whl/torchdata-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.2.3)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.2)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.15.2) (2024.12.14)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Building wheels for collected packages: lit\n",
            "  Building wheel for lit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89991 sha256=8f8c790232a83d04fd4578f743579fcbef00f73400f3838e73aaa039a9214188\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/2c/b6/3ed2983b1b44fe0dea1bb35234b09f2c22fb8ebb308679c922\n",
            "Successfully built lit\n",
            "Installing collected packages: lit, triton, torch, torchdata, torchtext\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.0.1+cu118 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-15.0.7 torch-2.0.1+cu118 torchdata-0.6.1 torchtext-0.15.2+cpu triton-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torch torchtext -y\n",
        "!pip install torch==2.0.1 torchtext==0.15.2 --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install portalocker>=2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JUNm_Cl2bmT"
      },
      "outputs": [],
      "source": [
        "from torchtext import datasets\n",
        "from torchtext.data import to_map_style_dataset\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "train_iter, test_iter = datasets.AG_NEWS(split=('train', 'test'))\n",
        "\n",
        "train_ds = to_map_style_dataset(train_iter)\n",
        "test_ds = to_map_style_dataset(test_iter)\n",
        "\n",
        "train = np.array(train_ds)\n",
        "test = np.array(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTuk1Z-42bmV"
      },
      "source": [
        "#### **Tokenización**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojB9LRww2bmV"
      },
      "outputs": [],
      "source": [
        "# Create vocabulary and embedding\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "vocab = build_vocab_from_iterator(map(lambda x: tokenizer(x[1]), train_iter), specials=['<pad>','<unk>'])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc2TzG602bmV",
        "outputId": "df27f818-2575-4824-88fd-5ac8e21a2870"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamaño del vocabulario: 95812 tokens\n",
            "Tokenización de la frase 'Here is an example sentence': ['here', 'is', 'an', 'example', 'sentence']\n",
            "Índices de las palabras 'here', 'is', 'an', 'example', 'supercalifragilisticexpialidocious': [476, 22, 31, 5298, 1]\n",
            "Palabras correspondientes a los índices 475, 21, 30, 5297, 0: ['version', 'at', 'from', 'establish', '<pad>']\n",
            "Las diez primeras palabras del vocabulario: ['<pad>', '<unk>', '.', 'the', ',', 'to', 'a', 'of', 'in', 'and']\n"
          ]
        }
      ],
      "source": [
        "print(\"Tamaño del vocabulario:\", len(vocab), \"tokens\")\n",
        "print(\"Tokenización de la frase 'Here is an example sentence':\", tokenizer(\"Here is an example sentence\"))\n",
        "print(\"Índices de las palabras 'here', 'is', 'an', 'example', 'supercalifragilisticexpialidocious':\", vocab(['here', 'is', 'an', 'example', 'supercalifragilisticexpialidocious']))\n",
        "print(\"Palabras correspondientes a los índices 475, 21, 30, 5297, 0:\", vocab.lookup_tokens([475, 21, 30, 5297, 0]))\n",
        "print(\"Las diez primeras palabras del vocabulario:\", vocab.get_itos()[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGvMntzl2bmV",
        "outputId": "a541a8d4-4fe5-49a0-e7ca-a398293ba7d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenización de la frase 'Here is an example sentence': [476, 22, 31, 5298, 2994]\n"
          ]
        }
      ],
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1\n",
        "\n",
        "print(\"Tokenización de la frase 'Here is an example sentence':\", text_pipeline(\"Here is an example sentence\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA2fdigm2bmW"
      },
      "source": [
        "#### **DataLoader**\n",
        "\n",
        "En PyTorch, **DataLoader** se utiliza para cargar y manejar datos de manera eficiente durante el entrenamiento de modelos de aprendizaje profundo, especialmente en tareas de aprendizaje supervisado como la clasificación, la regresión y más. El DataLoader forma parte de la biblioteca torch.utils.data, y su objetivo principal es facilitar la administración de lotes (batches) de datos y la distribución de esos lotes al modelo de forma automática.\n",
        "\n",
        "La función <code>collate_batch</code> es una función personalizada que se utiliza como argumento para collate_fn al crear instancias de los objetos DataLoader. Tiene la responsabilidad de procesar y agrupar las muestras individuales de datos dentro de un lote (batch) de manera que sean compatibles para su posterior procesamiento dentro de la LSTM. Es especialmente útil cuando las secuencias de texto tienen longitudes diferentes y es necesario realizar un relleno (padding) para que todas tengan la misma longitud."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4YxW_oH2bmW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for sample in batch:\n",
        "        label, text = sample\n",
        "        text_list.append(torch.tensor(text_pipeline(text), dtype=torch.long))\n",
        "        label_list.append(label_pipeline(label))\n",
        "    return torch.tensor(label_list, dtype=torch.long), torch.nn.utils.rnn.pad_sequence(text_list, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_iter, batch_size=64, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_iter, batch_size=64, shuffle=True, collate_fn=collate_batch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLB2kCjh2bmW"
      },
      "source": [
        "Para verificar que estamos creando correctamente los lotes, vamos a imprimir las primeras cuatro instancias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaBBazFY2bmW",
        "outputId": "7f8be493-a790-481c-bf93-916f89867fd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[  640,   431, 16431,     7,   222,  3295,   576,     7,   337,   326,\n",
            "           114,    91,    50,    36,     6,   640, 20317,    21, 14357,     4,\n",
            "            50,   704,    44,    12,  1490,    62,   133,   178,    20,     6,\n",
            "          4115,  2664,  4700,     6,    24,  5726,  1032,  7885,    19,    31,\n",
            "         21040,  7328,    18,  2376,  5970,   480,     8,     3, 21097,     2,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [ 7645,    51,    17,   743,  6725,   712,  7391,    17,   103,    34,\n",
            "           799,     5,  6725,    47, 15204,     9,   712,  7391,   303,     7,\n",
            "         10694,  1067,  1310,     4,   454,   234,     2,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [   17,  7631,   218,    17,    90,  2303,    12,  3017,    53,    72,\n",
            "            16,  2066,   592,    14,    28,    15,    28,    16,     3,  3017,\n",
            "           475,     7,  7631,   218,     4, 51678,   405,   399,    90,  1415,\n",
            "           345,     5,  1882,  9014,     4, 23421,    63,   588,     5,    38,\n",
            "           345,     8,   141,  4870,     6,  1552, 51951,    90,   289,    76,\n",
            "            43, 12157,  6001,    26,     2,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [ 3707,  4999, 31987,  6578,   237,  3707,   541,   881,     7,   751,\n",
            "         31987, 17846,   456,     5,  1075,  4899, 15324,   136,     5, 14786,\n",
            "          6944,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "\n",
            "\n",
            "tensor([1, 3, 3, 3])\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for batch in train_dataloader:\n",
        "    print(batch[1][:4])\n",
        "    print(\"\\n\")\n",
        "    print(batch[0][:4])\n",
        "    print(\"\\n\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWkdxE4s2bmW"
      },
      "source": [
        "#### **Modelo LSTM**\n",
        "\n",
        "Definimos una clase llamada `LSTMTextClassificationModel` para la clasificación de texto basado en LSTM. Esta clase hereda de `nn.Module`, la clase base para todos los modelos en PyTorch.\n",
        "\n",
        "**Inicialización del Modelo**: En el método `__init__`, se definen los componentes principales del modelo y se configuran sus parámetros:\n",
        "   - **vocab_size**: El tamaño del vocabulario, es decir, la cantidad de palabras únicas en el conjunto de datos de entrenamiento.\n",
        "   - **embed_dim**: La dimensión de los vectores de embedding para representar las palabras.\n",
        "   - **hidden_dim**: El tamaño de la capa oculta de la LSTM, que controla la cantidad de unidades o \"memoria\" en la red LSTM.\n",
        "   - **num_class**: El número de clases de salida en la tarea de clasificación.\n",
        "\n",
        "\n",
        "**Capa de Embedding**: Se define una capa de embedding (`nn.Embedding`) que se utilizará para representar las palabras como vectores densos. Esta capa no es pre-entrenada, lo que significa que los vectores de embedding se entrenarán junto con el modelo durante el proceso de entrenamiento.\n",
        "\n",
        "**Capa LSTM**: Se define una capa LSTM (`nn.LSTM`) que toma los vectores de embedding como entrada. `embed_dim` es la dimensión de entrada de la capa LSTM, y `hidden_dim` es la dimensión de su capa oculta. `batch_first=True` indica que los datos de entrada tendrán la forma `(batch_size, sequence_length, embed_dim)`.\n",
        "\n",
        "**Capa Fully Connected (FC)**: Se define una capa completamente conectada (`nn.Linear`) que se utiliza para producir las salidas de clasificación. Toma las salidas de la capa LSTM correspondientes a la última iteración y las reduce a `num_class` dimensiones, que corresponde al número de clases en la tarea de clasificación.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rymxc--a2bmX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMTextClassificationModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class):\n",
        "        super(LSTMTextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # <-- Capa de embedding genérica (no pre-entrenada)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_class)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)  # <-- Tras pasar por la capa de embedding, las palabras se representan como vectores\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        # Tomar la última salida de la secuencia LSTM\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "        output = self.fc(last_output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu5U7Zmr2bmX"
      },
      "source": [
        "Antes de seguir veamos cómo se conforma un *batch* de datos tras pasar la capa de *embedding*. Como se muestra en la figura siguiente, un *batch* es un vector de tres dimensiones, donde la primera dimensión es el tamaño del *batch*, la segunda dimensión es el número de palabras en cada secuencia y la tercera dimensión es el tamaño del vector de *embedding* (*channels* o *features*).\n",
        "\n",
        "<img src=\"imgs/Lote1_b.svg\" width=\"30%\">\n",
        "\n",
        "Por ejemplo, un *token* estaría ahora representado por un vector de *embedding* de tres componentes, en lugar de un escalar. Si lo quisiéramos referenciar sería: <code>mini_lote[0,3,:]</code>.\n",
        "\n",
        "<img src=\"imgs/Lote2_b.svg\" width=\"30%\">\n",
        "\n",
        "\n",
        "Fíjate que la tercera dimensión del tensor de salida de la LSTM no tiene por qué tener el mismo tamaño que la tercera dimensión del tensor de entrada. La parte del tensor correspondiente a la línea de código <code>last_output = lstm_out[:, -1, :]</code> sería parecida a la siguiente:\n",
        "\n",
        "<img src=\"imgs/Lote3_b.svg\" width=\"30%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmt-ZAho2bmX"
      },
      "source": [
        "Ya podemos pasar un primer mini-lote de datos por la red. Para ello, vamos a crearlo a partir de los datos de entrenamiento. Esto solo lo hacemos para asegurarnos de que el código funciona correctamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zalCgPse2bmX",
        "outputId": "3e26238c-603c-47e7-f6b9-9529b21f6e16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[    3,   270,  1683,  1668,     3,  3648, 37474,  7404,  1086,   391,\n",
            "          1828,    17,    10,   667,    43,   278,  8230,    25,  1879,   353,\n",
            "             7,     3,  3127,     4,  1291,     9,  2754,  3142,     2,    25,\n",
            "          1163, 87558,    30,  4608,  2109,     2,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0],\n",
            "        [ 2414,  5852,   470,     4,  3100,    12,  2730,   352,    54,    19,\n",
            "          9079,  2414,   467,    49,  1099,     5,    62,    64,    31,  3562,\n",
            "            76,  2993,     5,    38,  9149,     9,  2656,   861,    44, 16701,\n",
            "             4,     3,   277,   708,   621,    36,    33,     5,  1407,    75,\n",
            "             2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0],\n",
            "        [  176,    85,    50,    36,    11,  1957,  2213,    17,    10,  2864,\n",
            "            14,    32,    15,    32,    16,    31,   176,    11,   493,    12,\n",
            "          2245, 19389,   714,   820,  2006,     8,     6,   930,  1568,  1145,\n",
            "           115,     8,    33,    48,  1797,    30,  4627,    18,    50,    36,\n",
            "          1582,    11,     3,  5142,     7,  3009,  1957,  2213,     9,   159,\n",
            "          1036,   430,    91,    50,    36,   795,    11,  1233,   249,     2,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0],\n",
            "        [ 1211,     6,  9714,   247,    14,   575,   609,   614,    15,   575,\n",
            "           609,   614,    16, 61199,     4,   664, 13048, 35384, 95731, 89988,\n",
            "           123,  3471,   999,  1498,     5,   613,    21,     3, 11767, 12798,\n",
            "           446,   391,    49,  2559, 62911, 40222,   829,     3,  2310, 37540,\n",
            "          5205,   246,     5,  9164,     2,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0]])\n",
            "tensor([[ 0.1910, -0.0207, -0.0179,  0.1239],\n",
            "        [ 0.1910, -0.0207, -0.0179,  0.1239],\n",
            "        [ 0.1910, -0.0207, -0.0179,  0.1239],\n",
            "        [ 0.1910, -0.0207, -0.0179,  0.1239]], grad_fn=<SliceBackward0>)\n",
            "tensor([3, 1, 0, 0])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
            "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
          ]
        }
      ],
      "source": [
        "model = LSTMTextClassificationModel(len(vocab), 32, 64, 4)\n",
        "model.train()\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    predicted_label = model(batch[1])\n",
        "    label = batch[0]\n",
        "    break\n",
        "\n",
        "print(batch[1][:4])\n",
        "print(predicted_label[:4])\n",
        "print(label[:4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCuVuc8n2bmX"
      },
      "source": [
        "#### **Entrenamiento**\n",
        "\n",
        "Vamos a construir las funciones <code>train</code> y <code>evaluate</code>. Dentro de la función <code>evaluate</code> vamos a prestar atención al contexto <code>with torch.no_grad():</code>, que sirve para indicar que no se van a calcular los gradientes. Esto es así porque en la fase de evaluación no se van a actualizar los pesos de la red, solo se van a utilizar para calcular la precisión de la red. Esto optimiza los recursos utilizados por PyTorch en cuanto a la gestión de la memoria.\n",
        "\n",
        "Recuerda que en PyTorch, `model.train()` y `model.eval()` son métodos que se utilizan para cambiar el modo de entrenamiento de un modelo de aprendizaje profundo. Estos métodos afectan el comportamiento de ciertos módulos en el modelo, como las capas de dropout y normalización, que se comportan de manera diferente durante el entrenamiento y la evaluación. Específicamente hacen lo siguiente:\n",
        "\n",
        "**model.train()**:\n",
        "   - Cuando se llama a `model.train()`, el modelo se coloca en modo de entrenamiento.\n",
        "   - En este modo, las capas de dropout se activan, lo que significa que se aplican durante el entrenamiento para ayudar a evitar el sobreajuste. Durante el entrenamiento, las capas de dropout \"apagan\" aleatoriamente un porcentaje de las unidades (neuronas) en la red en cada paso de entrenamiento.\n",
        "   - También afecta a las capas de normalización, como Batch Normalization, para que utilicen estadísticas de mini-lotes (batches) durante el entrenamiento.\n",
        "\n",
        "**model.eval()**:\n",
        "   - Cuando se llama a `model.eval()`, el modelo se coloca en modo de evaluación o inferencia.\n",
        "   - En este modo, las capas de dropout se desactivan, lo que significa que no se aplican durante la evaluación. Esto garantiza que el modelo produzca resultados deterministas y coherentes durante la inferencia.\n",
        "   - Las capas de normalización, como Batch Normalization, utilizan estadísticas acumuladas durante el entrenamiento (en lugar de estadísticas de mini-lotes) para garantizar una evaluación coherente y precisa.\n",
        "\n",
        "\n",
        "En la práctica, es común utilizar `model.train()` antes de cada época de entrenamiento y `model.eval()` antes de realizar inferencia o evaluación en un modelo entrenado. Cambiar entre estos modos es esencial para garantizar que el modelo se comporte correctamente en diferentes etapas del proceso de entrenamiento y evaluación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vj5nL2R2bmX"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Hyperparameters\n",
        "EPOCHS = 10  # epoch\n",
        "LR = 5  # learning rate\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count, max_acc = 0, 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| {:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(idx, total_acc / total_count))\n",
        "\n",
        "            if max_acc < total_acc / total_count:\n",
        "                max_acc = total_acc / total_count\n",
        "\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "    return max_acc\n",
        "\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            predicted_label = model(text)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc / total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "i2kVie4Y2bmY",
        "outputId": "48435337-a4f0-4f28-c352-f037b53d054b"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    accu_train = train(train_dataloader)\n",
        "    accu_val = evaluate(test_dataloader)\n",
        "\n",
        "    #if accu_train > accu_val:\n",
        "    #    scheduler.step()\n",
        "\n",
        "    print(\"-\" * 59)\n",
        "    print(\n",
        "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
        "        \"valid accuracy {:8.3f} \".format(\n",
        "            epoch, time.time() - epoch_start_time, accu_val\n",
        "        )\n",
        "    )\n",
        "    print(\"-\" * 59)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFKC6Z-h2bmY"
      },
      "source": [
        "---\n",
        "\n",
        "## **Práctica 1: Uso de *embeddings* preentrenados**\n",
        "\n",
        "Modifica el código anterior para adaptar el modelo LSTM al uso de *embeddings* preentrenados. Para ello, usa <code>from torchtext.vocab import GloVe</code> y elige el conjunto de *embeddings* GloVe que prefieras. Puedes encontrar más información en https://pytorch.org/text/stable/vocab.html#torchtext.vocab.GloVe\n",
        "\n",
        "Verifica si se produce una mejora en la precisión del modelo. ¿Qué ocurre si usas un conjunto de *embeddings* preentrenados de diferentes tamaños?\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JEV9H2uMPmg"
      },
      "source": [
        "El uso de embeddings preentrenados de mayor dimensión mejora la precisión del modelo levemente, aunque el impacto es más significativo al pasar de dimensiones más bajas que entre dimensiones altas. Además, emplear modelos GloVe más grandes, como el 42B o el 840B, puede proporcionar embeddings más contextualizados debido al mayor volumen de datos en los que han sido entrenados, sin embargo dependiendo de la tarea en la que se vaya a utilizar puede ser demasiado, pudiendo haber modelos preentrenados más pequeños que funcionen correctamente sin la necesidad de modelos grandes que aumenten el coste computacional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbZC7hyulpy8"
      },
      "outputs": [],
      "source": [
        "from torchtext import datasets\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# Cargar el conjunto de datos AG_NEWS\n",
        "train_iter, test_iter = datasets.AG_NEWS(split=('train', 'test'))\n",
        "train_ds = to_map_style_dataset(train_iter)\n",
        "test_ds = to_map_style_dataset(test_iter)\n",
        "\n",
        "# Tokenizador y vocabulario\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "vocab = build_vocab_from_iterator(\n",
        "    map(lambda x: tokenizer(x[1]), train_iter), specials=['<pad>', '<unk>']\n",
        ")\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSfcxC7jlyRO",
        "outputId": "9aef0d42-8e08-4af0-ebfb-0405f17ff81f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando GloVe de dimensión 50...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:38, 5.43MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:13<00:00, 30701.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando GloVe de dimensión 100...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 399999/400000 [00:20<00:00, 19508.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando GloVe de dimensión 200...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 399999/400000 [00:35<00:00, 11174.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando GloVe de dimensión 300...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 399999/400000 [00:52<00:00, 7631.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings GloVe generados y almacenados.\n"
          ]
        }
      ],
      "source": [
        "from torchtext.vocab import GloVe\n",
        "\n",
        "# Crear y almacenar embeddings GloVe\n",
        "def create_and_store_glove_embeddings(vocab):\n",
        "    glove_embeddings = {}\n",
        "    for dim in [50, 100, 200, 300]:\n",
        "        print(f\"Cargando GloVe de dimensión {dim}...\")\n",
        "        glove = GloVe(name=\"6B\", dim=dim)\n",
        "        pretrained_embeddings = torch.zeros((len(vocab), glove.dim))\n",
        "        for idx, word in enumerate(vocab.get_itos()):\n",
        "            if word in glove.stoi:\n",
        "                pretrained_embeddings[idx] = glove[word]\n",
        "            else:\n",
        "                pretrained_embeddings[idx] = torch.randn(glove.dim)\n",
        "        glove_embeddings[dim] = pretrained_embeddings\n",
        "    print(\"Embeddings GloVe generados y almacenados.\")\n",
        "    return glove_embeddings\n",
        "\n",
        "# Almacenar los embeddings GloVe\n",
        "glove_embeddings = create_and_store_glove_embeddings(vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-q38SfFl1eJ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Modelo LSTM para clasificación de texto\n",
        "class LSTMTextClassificationModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class, pretrained_embeddings):\n",
        "        super(LSTMTextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, num_class)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "        output = self.fc(last_output)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aloc0iKNl3Iq"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Preparar DataLoader\n",
        "def collate_batch(batch):\n",
        "    labels, texts = zip(*batch)\n",
        "    labels = torch.tensor([int(label) - 1 for label in labels], dtype=torch.long).to(device)\n",
        "    texts = [torch.tensor(vocab(tokenizer(text)), dtype=torch.long).to(device) for text in texts]\n",
        "    texts = pad_sequence(texts, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
        "    return labels, texts\n",
        "\n",
        "train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_ds, batch_size=32, collate_fn=collate_batch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZDd7sbjl7LS"
      },
      "outputs": [],
      "source": [
        "# Configurar el modelo con embeddings de una dimensión específica\n",
        "def configure_model(dim, glove_embeddings):\n",
        "    pretrained_embeddings = glove_embeddings[dim]\n",
        "    model = LSTMTextClassificationModel(\n",
        "        vocab_size=len(vocab),\n",
        "        embed_dim=dim,\n",
        "        hidden_dim=128,\n",
        "        num_class=4,\n",
        "        pretrained_embeddings=pretrained_embeddings,\n",
        "    ).to(device)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4WdO_W_l5QD"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, optimizer):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predicted_label = model(text)\n",
        "        loss = criterion(predicted_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| {:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(idx, total_acc / total_count))\n",
        "            start_time = time.time()\n",
        "\n",
        "    return total_acc / total_count\n",
        "\n",
        "def evaluate(dataloader, model):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text) in enumerate(dataloader):\n",
        "            predicted_label = model(text)\n",
        "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "\n",
        "    return total_acc / total_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USkP0ttrl9-T"
      },
      "outputs": [],
      "source": [
        "# Hiperparámetros\n",
        "EPOCHS = 10\n",
        "LR = 5\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "results = []\n",
        "\n",
        "# Entrenar con una dimensión específica\n",
        "def train_with_dimension(dim):\n",
        "    print(f\"\\nEntrenando con GloVe de dimensión {dim}...\")\n",
        "    model = configure_model(dim, glove_embeddings)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
        "\n",
        "    # Entrenamiento con medición de tiempo\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        accu_train = train(train_dataloader, model, optimizer)\n",
        "        accu_val = evaluate(test_dataloader, model)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(\"-\" * 59)\n",
        "        print(\n",
        "            \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
        "            \"Train Acc: {:8.3f} | Valid Acc: {:8.3f}\".format(\n",
        "                epoch, time.time() - epoch_start_time, accu_train, accu_val\n",
        "            )\n",
        "        )\n",
        "        print(\"-\" * 59)\n",
        "\n",
        "        # Almacenar resultados finales de esta dimensión\n",
        "        if epoch == EPOCHS:  # Solo guardar el resultado al final de la última época\n",
        "            results.append({\n",
        "                \"dimension\": dim,\n",
        "                \"train_accuracy\": accu_train,\n",
        "                \"valid_accuracy\": accu_val,\n",
        "                \"time_per_epoch\": time.time() - epoch_start_time\n",
        "            })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gmEP-immA1p",
        "outputId": "ef8a7363-b91f-49d0-80bf-a4967ea32922"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entrenando con GloVe de dimensión 50...\n",
            "|   500 batches | accuracy    0.252\n",
            "|  1000 batches | accuracy    0.274\n",
            "|  1500 batches | accuracy    0.406\n",
            "|  2000 batches | accuracy    0.493\n",
            "|  2500 batches | accuracy    0.566\n",
            "|  3000 batches | accuracy    0.615\n",
            "|  3500 batches | accuracy    0.652\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 21.67s | Train Acc:    0.667 | Valid Acc:    0.866\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.883\n",
            "|  1000 batches | accuracy    0.888\n",
            "|  1500 batches | accuracy    0.888\n",
            "|  2000 batches | accuracy    0.889\n",
            "|  2500 batches | accuracy    0.889\n",
            "|  3000 batches | accuracy    0.889\n",
            "|  3500 batches | accuracy    0.890\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 21.56s | Train Acc:    0.890 | Valid Acc:    0.888\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.893\n",
            "|  1000 batches | accuracy    0.893\n",
            "|  1500 batches | accuracy    0.894\n",
            "|  2000 batches | accuracy    0.895\n",
            "|  2500 batches | accuracy    0.896\n",
            "|  3000 batches | accuracy    0.895\n",
            "|  3500 batches | accuracy    0.895\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 21.46s | Train Acc:    0.895 | Valid Acc:    0.892\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.893\n",
            "|  1000 batches | accuracy    0.895\n",
            "|  1500 batches | accuracy    0.896\n",
            "|  2000 batches | accuracy    0.896\n",
            "|  2500 batches | accuracy    0.896\n",
            "|  3000 batches | accuracy    0.896\n",
            "|  3500 batches | accuracy    0.896\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 21.14s | Train Acc:    0.896 | Valid Acc:    0.892\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.894\n",
            "|  1000 batches | accuracy    0.895\n",
            "|  1500 batches | accuracy    0.895\n",
            "|  2000 batches | accuracy    0.895\n",
            "|  2500 batches | accuracy    0.895\n",
            "|  3000 batches | accuracy    0.896\n",
            "|  3500 batches | accuracy    0.896\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 21.00s | Train Acc:    0.896 | Valid Acc:    0.893\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.896\n",
            "|  1000 batches | accuracy    0.896\n",
            "|  1500 batches | accuracy    0.894\n",
            "|  2000 batches | accuracy    0.895\n",
            "|  2500 batches | accuracy    0.895\n",
            "|  3000 batches | accuracy    0.895\n",
            "|  3500 batches | accuracy    0.896\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 21.31s | Train Acc:    0.896 | Valid Acc:    0.893\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.895\n",
            "|  1000 batches | accuracy    0.897\n",
            "|  1500 batches | accuracy    0.896\n",
            "|  2000 batches | accuracy    0.896\n",
            "|  2500 batches | accuracy    0.895\n",
            "|  3000 batches | accuracy    0.896\n",
            "|  3500 batches | accuracy    0.896\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 21.32s | Train Acc:    0.896 | Valid Acc:    0.893\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.898\n",
            "|  1000 batches | accuracy    0.899\n",
            "|  1500 batches | accuracy    0.899\n",
            "|  2000 batches | accuracy    0.898\n",
            "|  2500 batches | accuracy    0.896\n",
            "|  3000 batches | accuracy    0.896\n",
            "|  3500 batches | accuracy    0.896\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 20.64s | Train Acc:    0.896 | Valid Acc:    0.893\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.893\n",
            "|  1000 batches | accuracy    0.895\n",
            "|  1500 batches | accuracy    0.896\n",
            "|  2000 batches | accuracy    0.897\n",
            "|  2500 batches | accuracy    0.896\n",
            "|  3000 batches | accuracy    0.896\n",
            "|  3500 batches | accuracy    0.896\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 21.51s | Train Acc:    0.896 | Valid Acc:    0.893\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.895\n",
            "|  1000 batches | accuracy    0.893\n",
            "|  1500 batches | accuracy    0.895\n",
            "|  2000 batches | accuracy    0.896\n",
            "|  2500 batches | accuracy    0.896\n",
            "|  3000 batches | accuracy    0.896\n",
            "|  3500 batches | accuracy    0.896\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 21.53s | Train Acc:    0.896 | Valid Acc:    0.893\n",
            "-----------------------------------------------------------\n",
            "\n",
            "Entrenando con GloVe de dimensión 100...\n",
            "|   500 batches | accuracy    0.250\n",
            "|  1000 batches | accuracy    0.400\n",
            "|  1500 batches | accuracy    0.543\n",
            "|  2000 batches | accuracy    0.625\n",
            "|  2500 batches | accuracy    0.675\n",
            "|  3000 batches | accuracy    0.710\n",
            "|  3500 batches | accuracy    0.735\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 21.38s | Train Acc:    0.746 | Valid Acc:    0.879\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.900\n",
            "|  1000 batches | accuracy    0.903\n",
            "|  1500 batches | accuracy    0.902\n",
            "|  2000 batches | accuracy    0.902\n",
            "|  2500 batches | accuracy    0.902\n",
            "|  3000 batches | accuracy    0.903\n",
            "|  3500 batches | accuracy    0.903\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 20.65s | Train Acc:    0.904 | Valid Acc:    0.899\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.908\n",
            "|  1000 batches | accuracy    0.910\n",
            "|  1500 batches | accuracy    0.909\n",
            "|  2000 batches | accuracy    0.909\n",
            "|  2500 batches | accuracy    0.909\n",
            "|  3000 batches | accuracy    0.909\n",
            "|  3500 batches | accuracy    0.908\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 21.56s | Train Acc:    0.908 | Valid Acc:    0.898\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.910\n",
            "|  1000 batches | accuracy    0.908\n",
            "|  1500 batches | accuracy    0.909\n",
            "|  2000 batches | accuracy    0.909\n",
            "|  2500 batches | accuracy    0.909\n",
            "|  3000 batches | accuracy    0.909\n",
            "|  3500 batches | accuracy    0.909\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 21.40s | Train Acc:    0.909 | Valid Acc:    0.899\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.910\n",
            "|  1000 batches | accuracy    0.908\n",
            "|  1500 batches | accuracy    0.908\n",
            "|  2000 batches | accuracy    0.909\n",
            "|  2500 batches | accuracy    0.909\n",
            "|  3000 batches | accuracy    0.909\n",
            "|  3500 batches | accuracy    0.909\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 20.60s | Train Acc:    0.909 | Valid Acc:    0.899\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.909\n",
            "|  1000 batches | accuracy    0.909\n",
            "|  1500 batches | accuracy    0.908\n",
            "|  2000 batches | accuracy    0.909\n",
            "|  2500 batches | accuracy    0.909\n",
            "|  3000 batches | accuracy    0.909\n",
            "|  3500 batches | accuracy    0.909\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 21.38s | Train Acc:    0.909 | Valid Acc:    0.899\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.907\n",
            "|  1000 batches | accuracy    0.909\n",
            "|  1500 batches | accuracy    0.907\n",
            "|  2000 batches | accuracy    0.907\n",
            "|  2500 batches | accuracy    0.909\n",
            "|  3000 batches | accuracy    0.908\n",
            "|  3500 batches | accuracy    0.909\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 21.24s | Train Acc:    0.909 | Valid Acc:    0.899\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.910\n",
            "|  1000 batches | accuracy    0.911\n",
            "|  1500 batches | accuracy    0.910\n",
            "|  2000 batches | accuracy    0.909\n",
            "|  2500 batches | accuracy    0.909\n",
            "|  3000 batches | accuracy    0.909\n",
            "|  3500 batches | accuracy    0.909\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 21.17s | Train Acc:    0.909 | Valid Acc:    0.899\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.909\n",
            "|  1000 batches | accuracy    0.909\n",
            "|  1500 batches | accuracy    0.909\n",
            "|  2000 batches | accuracy    0.909\n",
            "|  2500 batches | accuracy    0.909\n",
            "|  3000 batches | accuracy    0.909\n",
            "|  3500 batches | accuracy    0.909\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 20.81s | Train Acc:    0.909 | Valid Acc:    0.899\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.912\n",
            "|  1000 batches | accuracy    0.912\n",
            "|  1500 batches | accuracy    0.911\n",
            "|  2000 batches | accuracy    0.910\n",
            "|  2500 batches | accuracy    0.909\n",
            "|  3000 batches | accuracy    0.910\n",
            "|  3500 batches | accuracy    0.909\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 21.43s | Train Acc:    0.909 | Valid Acc:    0.899\n",
            "-----------------------------------------------------------\n",
            "\n",
            "Entrenando con GloVe de dimensión 200...\n",
            "|   500 batches | accuracy    0.255\n",
            "|  1000 batches | accuracy    0.507\n",
            "|  1500 batches | accuracy    0.626\n",
            "|  2000 batches | accuracy    0.689\n",
            "|  2500 batches | accuracy    0.727\n",
            "|  3000 batches | accuracy    0.754\n",
            "|  3500 batches | accuracy    0.773\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 21.60s | Train Acc:    0.781 | Valid Acc:    0.894\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.906\n",
            "|  1000 batches | accuracy    0.907\n",
            "|  1500 batches | accuracy    0.908\n",
            "|  2000 batches | accuracy    0.909\n",
            "|  2500 batches | accuracy    0.909\n",
            "|  3000 batches | accuracy    0.909\n",
            "|  3500 batches | accuracy    0.910\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 20.82s | Train Acc:    0.910 | Valid Acc:    0.908\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.914\n",
            "|  1000 batches | accuracy    0.916\n",
            "|  1500 batches | accuracy    0.916\n",
            "|  2000 batches | accuracy    0.915\n",
            "|  2500 batches | accuracy    0.916\n",
            "|  3000 batches | accuracy    0.915\n",
            "|  3500 batches | accuracy    0.916\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 21.72s | Train Acc:    0.916 | Valid Acc:    0.908\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.915\n",
            "|  1000 batches | accuracy    0.916\n",
            "|  1500 batches | accuracy    0.915\n",
            "|  2000 batches | accuracy    0.915\n",
            "|  2500 batches | accuracy    0.916\n",
            "|  3000 batches | accuracy    0.916\n",
            "|  3500 batches | accuracy    0.916\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 21.70s | Train Acc:    0.916 | Valid Acc:    0.909\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.914\n",
            "|  1000 batches | accuracy    0.915\n",
            "|  1500 batches | accuracy    0.916\n",
            "|  2000 batches | accuracy    0.917\n",
            "|  2500 batches | accuracy    0.917\n",
            "|  3000 batches | accuracy    0.917\n",
            "|  3500 batches | accuracy    0.917\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 21.55s | Train Acc:    0.917 | Valid Acc:    0.909\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.920\n",
            "|  1000 batches | accuracy    0.918\n",
            "|  1500 batches | accuracy    0.917\n",
            "|  2000 batches | accuracy    0.916\n",
            "|  2500 batches | accuracy    0.916\n",
            "|  3000 batches | accuracy    0.917\n",
            "|  3500 batches | accuracy    0.916\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 20.86s | Train Acc:    0.917 | Valid Acc:    0.909\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.914\n",
            "|  1000 batches | accuracy    0.914\n",
            "|  1500 batches | accuracy    0.916\n",
            "|  2000 batches | accuracy    0.916\n",
            "|  2500 batches | accuracy    0.916\n",
            "|  3000 batches | accuracy    0.917\n",
            "|  3500 batches | accuracy    0.916\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 21.54s | Train Acc:    0.916 | Valid Acc:    0.909\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.915\n",
            "|  1000 batches | accuracy    0.916\n",
            "|  1500 batches | accuracy    0.915\n",
            "|  2000 batches | accuracy    0.916\n",
            "|  2500 batches | accuracy    0.916\n",
            "|  3000 batches | accuracy    0.916\n",
            "|  3500 batches | accuracy    0.916\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 21.53s | Train Acc:    0.916 | Valid Acc:    0.909\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.916\n",
            "|  1000 batches | accuracy    0.916\n",
            "|  1500 batches | accuracy    0.916\n",
            "|  2000 batches | accuracy    0.917\n",
            "|  2500 batches | accuracy    0.917\n",
            "|  3000 batches | accuracy    0.917\n",
            "|  3500 batches | accuracy    0.917\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 21.16s | Train Acc:    0.917 | Valid Acc:    0.909\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.918\n",
            "|  1000 batches | accuracy    0.918\n",
            "|  1500 batches | accuracy    0.916\n",
            "|  2000 batches | accuracy    0.916\n",
            "|  2500 batches | accuracy    0.917\n",
            "|  3000 batches | accuracy    0.917\n",
            "|  3500 batches | accuracy    0.917\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 21.06s | Train Acc:    0.917 | Valid Acc:    0.909\n",
            "-----------------------------------------------------------\n",
            "\n",
            "Entrenando con GloVe de dimensión 300...\n",
            "|   500 batches | accuracy    0.358\n",
            "|  1000 batches | accuracy    0.611\n",
            "|  1500 batches | accuracy    0.702\n",
            "|  2000 batches | accuracy    0.748\n",
            "|  2500 batches | accuracy    0.777\n",
            "|  3000 batches | accuracy    0.796\n",
            "|  3500 batches | accuracy    0.811\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 21.46s | Train Acc:    0.816 | Valid Acc:    0.897\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.909\n",
            "|  1000 batches | accuracy    0.911\n",
            "|  1500 batches | accuracy    0.912\n",
            "|  2000 batches | accuracy    0.912\n",
            "|  2500 batches | accuracy    0.912\n",
            "|  3000 batches | accuracy    0.913\n",
            "|  3500 batches | accuracy    0.914\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 21.46s | Train Acc:    0.914 | Valid Acc:    0.912\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.919\n",
            "|  1000 batches | accuracy    0.919\n",
            "|  1500 batches | accuracy    0.920\n",
            "|  2000 batches | accuracy    0.920\n",
            "|  2500 batches | accuracy    0.921\n",
            "|  3000 batches | accuracy    0.920\n",
            "|  3500 batches | accuracy    0.921\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 20.87s | Train Acc:    0.921 | Valid Acc:    0.912\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.925\n",
            "|  1000 batches | accuracy    0.922\n",
            "|  1500 batches | accuracy    0.921\n",
            "|  2000 batches | accuracy    0.922\n",
            "|  2500 batches | accuracy    0.921\n",
            "|  3000 batches | accuracy    0.922\n",
            "|  3500 batches | accuracy    0.922\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 21.41s | Train Acc:    0.922 | Valid Acc:    0.912\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.922\n",
            "|  1000 batches | accuracy    0.920\n",
            "|  1500 batches | accuracy    0.921\n",
            "|  2000 batches | accuracy    0.922\n",
            "|  2500 batches | accuracy    0.921\n",
            "|  3000 batches | accuracy    0.921\n",
            "|  3500 batches | accuracy    0.922\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 21.51s | Train Acc:    0.922 | Valid Acc:    0.912\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.922\n",
            "|  1000 batches | accuracy    0.922\n",
            "|  1500 batches | accuracy    0.922\n",
            "|  2000 batches | accuracy    0.922\n",
            "|  2500 batches | accuracy    0.922\n",
            "|  3000 batches | accuracy    0.922\n",
            "|  3500 batches | accuracy    0.922\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 21.53s | Train Acc:    0.922 | Valid Acc:    0.912\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.920\n",
            "|  1000 batches | accuracy    0.919\n",
            "|  1500 batches | accuracy    0.919\n",
            "|  2000 batches | accuracy    0.921\n",
            "|  2500 batches | accuracy    0.921\n",
            "|  3000 batches | accuracy    0.921\n",
            "|  3500 batches | accuracy    0.922\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 20.77s | Train Acc:    0.922 | Valid Acc:    0.912\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.922\n",
            "|  1000 batches | accuracy    0.921\n",
            "|  1500 batches | accuracy    0.922\n",
            "|  2000 batches | accuracy    0.922\n",
            "|  2500 batches | accuracy    0.921\n",
            "|  3000 batches | accuracy    0.922\n",
            "|  3500 batches | accuracy    0.922\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 21.50s | Train Acc:    0.922 | Valid Acc:    0.912\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.922\n",
            "|  1000 batches | accuracy    0.922\n",
            "|  1500 batches | accuracy    0.922\n",
            "|  2000 batches | accuracy    0.922\n",
            "|  2500 batches | accuracy    0.921\n",
            "|  3000 batches | accuracy    0.921\n",
            "|  3500 batches | accuracy    0.921\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 21.67s | Train Acc:    0.922 | Valid Acc:    0.912\n",
            "-----------------------------------------------------------\n",
            "|   500 batches | accuracy    0.929\n",
            "|  1000 batches | accuracy    0.925\n",
            "|  1500 batches | accuracy    0.924\n",
            "|  2000 batches | accuracy    0.923\n",
            "|  2500 batches | accuracy    0.921\n",
            "|  3000 batches | accuracy    0.922\n",
            "|  3500 batches | accuracy    0.922\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 21.14s | Train Acc:    0.922 | Valid Acc:    0.912\n",
            "-----------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Llamar al entrenamiento con diferentes dimensiones\n",
        "train_with_dimension(50)\n",
        "train_with_dimension(100)\n",
        "train_with_dimension(200)\n",
        "train_with_dimension(300)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf2vt-Cyx0bE",
        "outputId": "eac24ae3-791d-4dae-9dbb-19ceeb5fe602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Resultados comparativos:\n",
            "Dimensión:  50 | Train Acc:    0.896 | Valid Acc:    0.893 | Tiempo por última época: 21.53s\n",
            "Dimensión: 100 | Train Acc:    0.909 | Valid Acc:    0.899 | Tiempo por última época: 21.44s\n",
            "Dimensión: 200 | Train Acc:    0.917 | Valid Acc:    0.909 | Tiempo por última época: 21.06s\n",
            "Dimensión: 300 | Train Acc:    0.922 | Valid Acc:    0.912 | Tiempo por última época: 21.14s\n"
          ]
        }
      ],
      "source": [
        "# Imprimir los resultados finales\n",
        "print(\"\\nResultados comparativos:\")\n",
        "for result in results:\n",
        "    print(\n",
        "        \"Dimensión: {:3d} | Train Acc: {:8.3f} | Valid Acc: {:8.3f} | Tiempo por última época: {:5.2f}s\".format(\n",
        "            result[\"dimension\"], result[\"train_accuracy\"], result[\"valid_accuracy\"], result[\"time_per_epoch\"]\n",
        "        )\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
