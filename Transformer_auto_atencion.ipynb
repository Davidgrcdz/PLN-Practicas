{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "Q = torch.tensor([[0.0, 0.0, 0.0], [1, 1, 1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3]])\n",
    "K = torch.tensor([[0.1, 0.1, 0.1], [0.2, 0.2, 0.2], [0.3, 0.3, 0.3], [0.4, 0.4, 0.4]])\n",
    "V = torch.tensor([[[1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 1.]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3000, 0.6000, 0.9000, 1.2000],\n",
      "        [0.0600, 0.1200, 0.1800, 0.2400],\n",
      "        [0.0900, 0.1800, 0.2700, 0.3600]])\n"
     ]
    }
   ],
   "source": [
    "scores = torch.matmul(Q, K.transpose(0, 1))\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000,   -inf,   -inf,   -inf],\n",
      "        [0.3000, 0.6000,   -inf,   -inf],\n",
      "        [0.0600, 0.1200, 0.1800,   -inf],\n",
      "        [0.0900, 0.1800, 0.2700, 0.3600]])\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos la máscara\n",
    "# Usa la máscara para hacer que los valores de las posiciones que no queremos que se tengan en cuenta sean -infinito, de manera que al aplicar la softmax, estos valores se conviertan en 0, y no les damos importancia.\n",
    "\n",
    "mask = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0], [1, 1, 1, 1]])\n",
    "masked_scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "print(masked_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000,   -inf,   -inf,   -inf],\n",
      "        [0.1732, 0.3464,   -inf,   -inf],\n",
      "        [0.0346, 0.0693, 0.1039,   -inf],\n",
      "        [0.0520, 0.1039, 0.1559, 0.2078]])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Dividimos por la raíz cuadrada de la dimensión de la matriz K\n",
    "sqrt_dk = math.sqrt(K.size(-1))\n",
    "scaled_scores = masked_scores / sqrt_dk\n",
    "print(scaled_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4568, 0.5432, 0.0000, 0.0000],\n",
      "        [0.3219, 0.3332, 0.3449, 0.0000],\n",
      "        [0.2309, 0.2432, 0.2561, 0.2698]])\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos la softmax\n",
    "attention = torch.nn.functional.softmax(scaled_scores, dim=-1)\n",
    "print(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.0000, 0.0000],\n",
      "         [0.4568, 0.5432, 0.0000],\n",
      "         [0.3219, 0.3332, 0.3449],\n",
      "         [0.2309, 0.5130, 0.5260]]])\n"
     ]
    }
   ],
   "source": [
    "# Multiplicamos por la matriz V\n",
    "Z = torch.matmul(attention, V)\n",
    "print(Z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pln",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
